The idea of text-to-image synthesis has been a very popular topic in the field of Deep Learning and many approaches exist for generating images based on the textual descriptions provided which their some advantages as well as some limitations. Autoencoders were used initially for generating images but it had some limitations. They were only efficient in generating images that were similar to the images they were trained on and faced difficulty in generating images which differ a lot from the images of the training set and as such on a description which is completely different from the training set would result to an image which would not correspond to the provided description. The introduction of Generative Adversarial Networks(GANs) in 2014 by Ian Goodfellow brought a remarkable change into the field of image generation and was able to generate images of much better resolution and details than that produced by earlier methods. It too suffer from some limitations which was on trying to generate images of higher resolution the images seem to deviate away from the descriptions provided. And also GANs are very hard to train. It also sometimes from non-convergence and the model parameters keep on oscillating and never converge, and results in generating a much lower resolution image. In Spite of these few drawbacks, GAN still seems to do a far better job at generating images than the other existing methods used.
	Moreover, no work has been done using GANs for generating images of a particular domain. All the work has been done for images which are of a range of domains and did not focus on generating specific images. Our idea is to focus mainly on the human faces and generating images based on the descriptions provided, by trying to come up with a new architecture or trying different combinations of already existing GAN models with some tweaks to get better, more detailed and higher resolution images. 